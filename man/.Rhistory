)
plot(predict(model1a, as.matrix(chicago_x)),type="l")
knitr::opts_chunk$set(echo = TRUE)
library(keras)
max_features <- 10000
imdb <- dataset_imdb(num_words = max_features)
imdb$train$x[[22]]
word_index<-dataset_imdb_word_index()
word_index<-do.call(c, word_index)
head(word_index)
review<-function(numbers){ lookup<-na.omit(match(numbers, word_index))
paste(names(word_index)[lookup],collapse=" ")
}
review(imdb$train$x[[22]])
str(imdb$train)
?dataset_imdb
review<-function(numbers){ lookup<-na.omit(match(numbers, word_index))
paste(names(word_index)[lookup],collapse=" ")
}
review(imdb$train$x[[22]])
library(keras)
max_features <- 10000
imdb <- dataset_imdb(num_words = max_features, index_from=1)
imdb$train$x[[22]]
word_index<-dataset_imdb_word_index()
word_index<-do.call(c, word_index)
head(word_index)
review<-function(numbers){ lookup<-na.omit(match(numbers, word_index))
paste(names(word_index)[lookup],collapse=" ")
}
review(imdb$train$x[[22]])
library(keras)
max_features <- 10000
imdb <- dataset_imdb(num_words = max_features, index_from=1, oov_char="%")
imdb$train$x[[22]]
log(10000)
log(10000,2)
model <- keras_model_sequential()
maxlen <- 400
embedding_dims <- 50
x_train <- imdb$train$x %>%
pad_sequences(maxlen = maxlen)
x_test <- imdb$test$x %>%
pad_sequences(maxlen = maxlen)
## for convolutional layer
filters <- 250
kernel_size <- 3
model %>%
# Start off with an efficient embedding layer which maps
# the vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
layer_dropout(0.4) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_dropout(0.4) %>%
layer_global_max_pooling_1d() %>%
layer_dense(250) %>%
layer_dropout(0.4) %>%
layer_activation("relu") %>%
# Project onto a single unit output layer, and squash it with a sigmoid
layer_dense(1) %>%
layer_activation("sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
batch_size <- 32
epochs <- 3
history <- model %>%
fit(
x_train, imdb$train$y,
batch_size = batch_size,
epochs = epochs,
validation_data = list(x_test, imdb$test$y)
)
library(keras)
max_features <- 10000
imdb <- dataset_imdb(num_words = max_features, index_from=1)
imdb$train$x[[22]]
word_index<-dataset_imdb_word_index()
word_index<-do.call(c, word_index)
head(word_index)
review<-function(numbers){ lookup<-na.omit(match(numbers, word_index))
paste(names(word_index)[lookup],collapse=" ")
}
review(imdb$train$x[[22]])
model <- keras_model_sequential()
maxlen <- 400
embedding_dims <- 50
x_train <- imdb$train$x %>%
pad_sequences(maxlen = maxlen)
x_test <- imdb$test$x %>%
pad_sequences(maxlen = maxlen)
## for convolutional layer
filters <- 250
kernel_size <- 3
model %>%
# Start off with an efficient embedding layer which maps
# the vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
layer_dropout(0.4) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_dropout(0.4) %>%
layer_global_max_pooling_1d() %>%
layer_dense(250) %>%
layer_dropout(0.4) %>%
layer_activation("relu") %>%
# Project onto a single unit output layer, and squash it with a sigmoid
layer_dense(1) %>%
layer_activation("sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
batch_size <- 32
epochs <- 3
history <- model %>%
fit(
x_train, imdb$train$y,
batch_size = batch_size,
epochs = epochs,
validation_data = list(x_test, imdb$test$y)
)
plot(history)
knitr::opts_chunk$set(echo = TRUE)
library(DBI)
library(dplyr, warn.conflicts = FALSE)
dbwords<-src_sqlite("WORDS/glove")
glove<-tbl(dbwords, "glove")
glove %>%
tally() %>%
collect()
glove
colnames(glove)
knitr::opts_chunk$set(echo = TRUE)
set.seed(2019-10-13)
library(DBI)
library(dplyr, warn.conflicts = FALSE)
dbwords<-src_sqlite("WORDS/glove")
glove<-tbl(dbwords, "glove")
glove %>%
tally() %>%
collect()
pos_words<-scan("WORDS/positive-words.txt", blank.lines.skip=TRUE,
comment.char=";", what="")
neg_words<-scan("WORDS/negative-words.txt", blank.lines.skip=TRUE,
comment.char=";", what="")
train_words<-tibble( word=c(pos_words,neg_words),
positive=rep(1:0,c(length(pos_words),length(neg_words))))
train_words<-copy_to(dbwords, train_words,
name="train_words",temporary=TRUE)
train <- inner_join(train_words, glove, by=c("word"="V1")) %>%
collect()
train_y<-train$positive
train_x<-train %>%
select(-word,-positive) %>%
as.matrix()
rownames(train_x)<-train$word
library(glmnet, quietly=TRUE)
test<-sample(nrow(train_x),nrow(train_x)/5)
fit<-cv.glmnet(train_x[-test,],train_y[-test],family="binomial", alpha=0.5 )
predicted<-predict(fit$glmnet.fit, train_x[test,],s=fit$lambda.min)
predicted[sample(nrow(predicted),20),]
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(tidyverse)
library(keras)
# Need to ignore 2nd row as it contains units only.
# Read in col names only
autism.names = read_csv("S1Dataset.csv", n_max = 0) %>% names()
autism.df = read_csv("S1Dataset.csv", col_names = autism.names, skip = 2)
autism.df = autism.df[,-26] # Drop Vineland ABC column
autism.subset = autism.df %>% filter(Group %in% c("ASD", "NEU"))
autism.sib = autism.df %>% filter(Group == "SIB")
autism.dunno<-autism.subset
for(i in 2:ncol(autism.dunno)){
autism.dunno[,i]<-runif(nrow(autism.dunno),min(autism.dunno[[i]]), max(autism.dunno[[i]]))
}
autism.both<-rbind(autism.subset,autism.dunno)
response.both = c(ifelse(autism.subset$Group == "ASD", 1, 0), rep(2,nrow(autism.dunno)))
preds = autism.both %>% select(-Group) %>% as.matrix() %>% scale()
preds.both = autism.both %>% select(-Group) %>% as.matrix() %>%
scale(center = attr(preds, "scaled:center"), scale = attr(preds, "scaled:scale"))
responseSib = ifelse(autism.sib$Group == "ASD", 1, 0)
model3 = keras_model_sequential()
model3 %>%
layer_dense(units = 8, activation = "relu", input_shape = dim(preds)[[2]]) %>%
layer_dense(units = 1, activation = "sigmoid")
summary(model3)
model3 %>% compile(
optimizer = optimizer_sgd(),
loss = c("binary_crossentropy"),
metrics = ("accuracy")
)
history = model3 %>% fit(
preds, response,
epochs = 500,
validation_split = 0.2,
verbose = 1
)
preds.both = autism.both %>% select(-Group) %>% as.matrix() %>% scale()
preds.sib = autism.sib %>% select(-Group) %>% as.matrix() %>%
scale(center = attr(preds, "scaled:center"), scale = attr(preds, "scaled:scale"))
responseSib = ifelse(autism.sib$Group == "ASD", 1, 0)
model3 = keras_model_sequential()
model3 %>%
layer_dense(units = 8, activation = "relu", input_shape = dim(preds)[[2]]) %>%
layer_dense(units = 1, activation = "sigmoid")
summary(model3)
model3 %>% compile(
optimizer = optimizer_sgd(),
loss = c("binary_crossentropy"),
metrics = ("accuracy")
)
history = model3 %>% fit(
preds.both, response.both,
epochs = 500,
validation_split = 0.2,
verbose = 1
)
preds.both = autism.both %>% select(-Group) %>% as.matrix() %>% scale()
preds.sib = autism.sib %>% select(-Group) %>% as.matrix() %>%
scale(center = attr(preds, "scaled:center"), scale = attr(preds, "scaled:scale"))
responseSib = ifelse(autism.sib$Group == "ASD", 1, 0)
model3 = keras_model_sequential()
model3 %>%
layer_dense(units = 8, activation = "relu", input_shape = dim(preds)[[2]]) %>%
layer_dense(units = 3, activation = "sigmoid")
summary(model3)
model3 %>% compile(
optimizer = optimizer_sgd(),
loss = c("categorical_crossentropy"),
metrics = ("accuracy")
)
history = model3 %>% fit(
preds.both, response.both,
epochs = 500,
validation_split = 0.2,
verbose = 1
)
preds.both = autism.both %>% select(-Group) %>% as.matrix() %>% scale()
preds.sib = autism.sib %>% select(-Group) %>% as.matrix() %>%
scale(center = attr(preds, "scaled:center"), scale = attr(preds, "scaled:scale"))
responseSib = ifelse(autism.sib$Group == "ASD", 1, 0)
model3 = keras_model_sequential()
model3 %>%
layer_dense(units = 8, activation = "relu", input_shape = dim(preds)[[2]]) %>%
layer_dense(units = 1, activation = "sigmoid")
summary(model3)
model3 %>% compile(
optimizer = optimizer_sgd(),
loss = c("sparse_categorical_crossentropy"),
metrics = ("accuracy")
)
history = model3 %>% fit(
preds.both, response.both,
epochs = 500,
validation_split = 0.2,
verbose = 1
)
preds.both = autism.both %>% select(-Group) %>% as.matrix() %>% scale()
preds.sib = autism.sib %>% select(-Group) %>% as.matrix() %>%
scale(center = attr(preds, "scaled:center"), scale = attr(preds, "scaled:scale"))
responseSib = ifelse(autism.sib$Group == "ASD", 1, 0)
model3 = keras_model_sequential()
model3 %>%
layer_dense(units = 8, activation = "relu", input_shape = dim(preds)[[2]]) %>%
layer_dense(units = 3, activation = "sigmoid")
summary(model3)
model3 %>% compile(
optimizer = optimizer_sgd(),
loss = c("sparse_categorical_crossentropy"),
metrics = ("accuracy")
)
history = model3 %>% fit(
preds.both, response.both,
epochs = 500,
validation_split = 0.2,
verbose = 1
)
plot(history)
score3out = model3 %>% predict(newdata=preds.sib)
?predict_classes
score3out = model3 %>% predict(x=preds.sib)
score3out
score3out = model3 %>% predict_classes(x=preds.sib)
score3out
score3out = model3 %>% predict_classes(x=preds.both)
score3out
score3out = model3 %>% predict_classes(x=preds.sib)
table(score3out, responseSib)
install.packages(c("tufte","formatR","bookdown"))
aliases<-list(Northland=c("Northland","NDHB"),
Waitemata=c("Waitemata","Waitematā"),
`Counties Manukau`=c("Counties Manukau","Counties","CM Health")
Taranaki=c("TDHB","Taranaki"),
Auckland=c("ADHB","Auckland"),
Waikato=c("Waikato"),
Whanganui=c("Whanganui","Wanganui","WDHB"),
`Capital and Coast`=c("Capital and Coast","Capital & Coast","CCDHB"),
`Bay of Plenty`=c("Bay of Plenty","Bay Of Plenty","BOP","BoP","BOPDHB"),
Lakes=c("Lakes"),
Midcentral=c("Midcentral","MidCentral","MDHB"),
Hutt = c("Hutt","Hutt Valley"),
Tairawhiti=c("Tairawhiti","Tairāwhiti"),
`Hawke's Bay`=c("Hawke's Bay","Hawkes Bay","HBDHB"),
Wairarapa=c("Wairarapa"),
`Nelson Marlborough`=c("Nelson Marlborough","Nelson-Marlborough","NMDHB"),
`West Coast`=c("West Coast","WCDHB"),
Canterbury=c("Canterbury","CDHB"),
`South Canterbury`=c("South Canterbury","SCDHB"),
Southern="Southern"
)
aliases<-list(Northland=c("Northland","NDHB"),
Waitemata=c("Waitemata","Waitematā"),
`Counties Manukau`=c("Counties Manukau","Counties","CM Health")
Taranaki=c("TDHB","Taranaki"),
Auckland=c("ADHB","Auckland"),
Waikato=c("Waikato"),
Whanganui=c("Whanganui","Wanganui","WDHB"),
`Capital and Coast`=c("Capital and Coast","Capital & Coast","CCDHB"),
`Bay of Plenty`=c("Bay of Plenty","Bay Of Plenty","BOP","BoP","BOPDHB"),
Lakes=c("Lakes"),
Midcentral=c("Midcentral","MidCentral","MDHB"),
Hutt = c("Hutt","Hutt Valley"),
Tairawhiti=c("Tairawhiti","Tairāwhiti"),
`Hawke's Bay`=c("Hawke's Bay","Hawkes Bay","HBDHB"),
Wairarapa=c("Wairarapa"),
`Nelson Marlborough`=c("Nelson Marlborough","Nelson-Marlborough","NMDHB"),
`West Coast`=c("West Coast","WCDHB"),
Canterbury=c("Canterbury","CDHB"),
`South Canterbury`=c("South Canterbury","SCDHB"),
Southern="Southern"
)
aliases<-list(Northland=c("Northland","NDHB"),
Waitemata=c("Waitemata","Waitematā"),
"Counties Manukau"=c("Counties Manukau","Counties","CM Health")
Taranaki=c("TDHB","Taranaki"),
Auckland=c("ADHB","Auckland"),
Waikato=c("Waikato"),
Whanganui=c("Whanganui","Wanganui","WDHB"),
"Capital and Coast"=c("Capital and Coast","Capital & Coast","CCDHB"),
"Bay of Plenty"=c("Bay of Plenty","Bay Of Plenty","BOP","BoP","BOPDHB"),
Lakes=c("Lakes"),
Midcentral=c("Midcentral","MidCentral","MDHB"),
Hutt = c("Hutt","Hutt Valley"),
Tairawhiti=c("Tairawhiti","Tairāwhiti"),
"Hawke's Bay"=c("Hawke's Bay","Hawkes Bay","HBDHB"),
Wairarapa=c("Wairarapa"),
"Nelson Marlborough"=c("Nelson Marlborough","Nelson-Marlborough","NMDHB"),
"West Coast"=c("West Coast","WCDHB"),
Canterbury=c("Canterbury","CDHB"),
`South Canterbury`=c("South Canterbury","SCDHB"),
Southern="Southern"
)
aliases<-list(Northland=c("Northland","NDHB"),
Waitemata=c("Waitemata","Waitematā"),
"Counties Manukau"=c("Counties Manukau","Counties","CM Health"),
Taranaki=c("TDHB","Taranaki"),
Auckland=c("ADHB","Auckland"),
Waikato=c("Waikato"),
Whanganui=c("Whanganui","Wanganui","WDHB"),
"Capital and Coast"=c("Capital and Coast","Capital & Coast","CCDHB"),
"Bay of Plenty"=c("Bay of Plenty","Bay Of Plenty","BOP","BoP","BOPDHB"),
Lakes=c("Lakes"),
Midcentral=c("Midcentral","MidCentral","MDHB"),
Hutt = c("Hutt","Hutt Valley"),
Tairawhiti=c("Tairawhiti","Tairāwhiti"),
"Hawke's Bay"=c("Hawke's Bay","Hawkes Bay","HBDHB"),
Wairarapa=c("Wairarapa"),
"Nelson Marlborough"=c("Nelson Marlborough","Nelson-Marlborough","NMDHB"),
"West Coast"=c("West Coast","WCDHB"),
Canterbury=c("Canterbury","CDHB"),
`South Canterbury`=c("South Canterbury","SCDHB"),
Southern="Southern"
)
aliases["Counties Manukau"]
aliases
do.call(aliases)
do.call(c,aliases)
?c
sapply(aliaes,length)
sapply(aliases,length)
aliases<-data.frame(keyname=rep(names(.aliases),sapply(.aliases,length)),
alias=do.call(c,.aliases,use.names=FALSE))
.aliases<-list(Northland=c("Northland","NDHB"),
Waitemata=c("Waitemata","Waitematā"),
"Counties Manukau"=c("Counties Manukau","Counties","CM Health"),
Taranaki=c("TDHB","Taranaki"),
Auckland=c("ADHB","Auckland"),
Waikato=c("Waikato"),
Whanganui=c("Whanganui","Wanganui","WDHB"),
"Capital and Coast"=c("Capital and Coast","Capital & Coast","CCDHB"),
"Bay of Plenty"=c("Bay of Plenty","Bay Of Plenty","BOP","BoP","BOPDHB"),
Lakes=c("Lakes"),
Midcentral=c("Midcentral","MidCentral","MDHB"),
Hutt = c("Hutt","Hutt Valley"),
Tairawhiti=c("Tairawhiti","Tairāwhiti"),
"Hawke's Bay"=c("Hawke's Bay","Hawkes Bay","HBDHB"),
Wairarapa=c("Wairarapa"),
"Nelson Marlborough"=c("Nelson Marlborough","Nelson-Marlborough","NMDHB"),
"West Coast"=c("West Coast","WCDHB"),
Canterbury=c("Canterbury","CDHB"),
`South Canterbury`=c("South Canterbury","SCDHB"),
Southern=c("Southern")
)
aliases<-data.frame(keyname=rep(names(.aliases),sapply(.aliases,length)),
alias=do.call(c,.aliases,use.names=FALSE))
aliases<-data.frame(keyname=rep(names(.aliases),sapply(.aliases,length)),
alias=do.call(c,c(.aliases,use.names=FALSE)))
aliases
source('~/DHBins/R/dhbins.R')
source('~/.active-rstudio-document')
traceback()
tris
dhb_lookup(rownames(tris))
dhb_lookup<-function(names){
idx<-match(names, aliases$alias)
if(any(is.na(idx)))
warning(paste("could not match",paste(names[is.na(idx)],collapse=",")))
canonical_name<-aliases$keyname[idx]
idx2<-match(canonical_name,dhbs$keyname)
idx2
}
dhb_lookup(rownames(tris))
debugSource('~/DHBins/R/dhbins.R')
source('~/DHBins/R/dhbins.R')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
dhtri(tri_colours=tris,legend=list(fill=c("green","gold","orange","goldenrod"),border=NA, legend=c("0","1","2","3+"),title="Cars/Household"))
par(mfrow=c(1,1))
dhtri(tri_colours=tris,legend=list(fill=c("green","gold","orange","goldenrod"),border=NA, legend=c("0","1","2","3+"),title="Cars/Household"))
households<-rowSums(cars[,-1])
names(households)<-cars$dhb
dhbin(radii=sqrt(households))
title(main="Number of households in private dwellings")
z<-rnorm(20)
z1<- (z+3)/6
col_z<-rgb(colorRamp(c("blue", "white","red"))(z1),max=255)
dhbin(hex_colours=col_z,border="grey",
legend_opts=list(fill=c("red","white","blue"),
legend=c("High","Medium","Low"),
title="Imaginary Index")
)
par(mfrow=c(2,3),mar=c(1,1,1,1))
for(i in 1:6){
z<-(rnorm(20)+z)/sqrt(2)
z1<- (z+3)/6
col_z<-rgb(colorRamp(c("blue", "white","red"))(z1),max=255)
dhbin(hex_colours=col_z,border="grey",
legend_opts=list(fill=c("red","white","blue"),
legend=c("High","Medium","Low"),
title=paste("Thing",i))
)
}
source('~/DHBins/R/dhbins.R')
source('~/DHBins/R/dhbins.R')
par(mfrow=c(2,3),mar=c(1,1,1,1))
for(i in 1:6){
z<-(rnorm(20)+z)/sqrt(2)
z1<- (z+3)/6
col_z<-rgb(colorRamp(c("blue", "white","red"))(z1),max=255)
dhbin(hex_colours=col_z,border="grey",short=TRUE,
legend_opts=list(fill=c("red","white","blue"),
legend=c("High","Medium","Low"),
title=paste("Thing",i))
)
}
source('~/DHBins/R/dhbins.R')
par(mfrow=c(2,3),mar=c(1,1,1,1))
for(i in 1:6){
z<-(rnorm(20)+z)/sqrt(2)
z1<- (z+3)/6
col_z<-rgb(colorRamp(c("blue", "white","red"))(z1),max=255)
dhbin(hex_colours=col_z,border="grey",short=TRUE,
legend_opts=list(fill=c("red","white","blue"),
legend=c("High","Medium","Low"),
title=paste("Thing",i))
)
}
getwd()
setwd("DHBins/man")
prompt(dhbin
)
prompt(ddhtri)
prompt(dhtri)
prompt(tri_alloc)
source('~/DHBins/R/dhbins.R')
par(mfrow=c(2,3),mar=c(1,1,1,1))
for(i in 1:6){
z<-(rnorm(20)+z)/sqrt(2)
z1<- (z+3)/6
col_z<-rgb(colorRamp(c("blue", "white","red"))(z1),max=255)
dhbin(hex_colours=col_z,border="grey",short=TRUE,
legend_opts=list(fill=c("red","white","blue"),
legend=c("High","Medium","Low"),
title=paste("Thing",i))
)
}
legend
cars
getwd()
dhb_cars<-cars
save(dhb_cars,file="../data/dhb_cars.rda")
prompt(dhb_cars)
